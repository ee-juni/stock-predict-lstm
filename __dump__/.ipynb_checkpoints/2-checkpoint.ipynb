{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as pdr\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num=0\n",
    "\n",
    "fn = ['DRB동일.csv', 'KEC.csv', 'SK아이이테크놀로지.csv', '기신정기.csv', '동양피스톤.csv', '두산퓨얼셀.csv', '디와이파워.csv', '삼아알미늄.csv', '티와이홀딩스.csv', '화승코퍼레이션.csv']\n",
    "\n",
    "df = pd.read_csv('data/aside/'+fn[num], index_col = 'Date', parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns='Volume')\n",
    "y = df.iloc[:, 4:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_line = len(y)-round(len(y)/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Shape (4930, 5) (4930, 1)\n",
      "Testing Shape (548, 5) (548, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "mm = MinMaxScaler()\n",
    "ss = StandardScaler()\n",
    "X_ss = ss.fit_transform(X)\n",
    "y_mm = mm.fit_transform(y)\n",
    "# Train Data\n",
    "X_train = X_ss[:cut_line, :]\n",
    "X_test = X_ss[cut_line:, :]\n",
    "# Test Data \n",
    "\"\"\" ( 굳이 없어도 된다. 하지만 얼마나 예측데이터와 실제 데이터의 정확도를 확인하기 위해 from sklearn.metrics import accuracy_score 를 통해 정확한 값으로 확인할 수 있다. ) \"\"\" \n",
    "y_train = y_mm[:cut_line, :]\n",
    "y_test = y_mm[cut_line:, :]\n",
    "print(\"Training Shape\", X_train.shape, y_train.shape)\n",
    "print(\"Testing Shape\", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Shape torch.Size([4930, 1, 5]) torch.Size([4930, 1])\n",
      "Testing Shape torch.Size([548, 1, 5]) torch.Size([548, 1])\n"
     ]
    }
   ],
   "source": [
    "X_train_tensors = Variable(torch.Tensor(X_train))\n",
    "X_test_tensors = Variable(torch.Tensor(X_test))\n",
    "y_train_tensors = Variable(torch.Tensor(y_train))\n",
    "y_test_tensors = Variable(torch.Tensor(y_test))\n",
    "\n",
    "X_train_tensors_final = torch.reshape(X_train_tensors, (X_train_tensors.shape[0], 1, X_train_tensors.shape[1]))\n",
    "X_test_tensors_final = torch.reshape(X_test_tensors, (X_test_tensors.shape[0], 1, X_test_tensors.shape[1]))\n",
    "print(\"Training Shape\", X_train_tensors_final.shape, y_train_tensors.shape)\n",
    "print(\"Testing Shape\", X_test_tensors_final.shape, y_test_tensors.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM1(nn.Module):\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers, seq_length):\n",
    "        super(LSTM1, self).__init__()\n",
    "        self.num_classes = num_classes #number of classes \n",
    "        self.num_layers = num_layers #number of layers \n",
    "        self.input_size = input_size #input size \n",
    "        self.hidden_size = hidden_size #hidden state \n",
    "        self.seq_length = seq_length #sequence length \n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True) #lstm \n",
    "        self.fc_1 = nn.Linear(hidden_size, 128) #fully connected 1 \n",
    "        self.fc = nn.Linear(128, num_classes) #fully connected last layer \n",
    "        self.relu = nn.ReLU() \n",
    "\n",
    "    def forward(self,x):\n",
    "        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)).to(device) #hidden state \n",
    "        c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)).to(device) #internal state \n",
    "        # Propagate input through LSTM \n",
    "\n",
    "        output, (hn, cn) = self.lstm(x, (h_0, c_0)) #lstm with input, hidden, and internal state \n",
    "        hn = hn.view(-1, self.hidden_size) #reshaping the data for Dense layer next \n",
    "        out = self.relu(hn) \n",
    "        out = self.fc_1(out) #first Dense \n",
    "        out = self.relu(out) #relu \n",
    "        out = self.fc(out) #Final Output \n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30000 #1000 epochs\n",
    "learning_rate = 0.0001 #0.001 lr\n",
    "\n",
    "input_size = 5 #number of features\n",
    "hidden_size = 2 #number of features in hidden state\n",
    "num_layers = 1 #number of stacked lstm layers\n",
    "\n",
    "num_classes = 1 #number of output classes \n",
    "lstm1 = LSTM1(num_classes, input_size, hidden_size, num_layers, X_train_tensors_final.shape[1]).to(device)\n",
    "\n",
    "loss_function = torch.nn.MSELoss()    # mean-squared error for regression\n",
    "optimizer = torch.optim.Adam(lstm1.parameters(), lr=learning_rate)  # adam optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 0.09687\n",
      "Epoch: 100, loss: 0.08193\n",
      "Epoch: 200, loss: 0.07874\n",
      "Epoch: 300, loss: 0.07516\n",
      "Epoch: 400, loss: 0.06850\n",
      "Epoch: 500, loss: 0.05514\n",
      "Epoch: 600, loss: 0.03691\n",
      "Epoch: 700, loss: 0.01992\n",
      "Epoch: 800, loss: 0.00987\n",
      "Epoch: 900, loss: 0.00562\n",
      "Epoch: 1000, loss: 0.00374\n",
      "Epoch: 1100, loss: 0.00268\n",
      "Epoch: 1200, loss: 0.00202\n",
      "Epoch: 1300, loss: 0.00158\n",
      "Epoch: 1400, loss: 0.00125\n",
      "Epoch: 1500, loss: 0.00101\n",
      "Epoch: 1600, loss: 0.00082\n",
      "Epoch: 1700, loss: 0.00066\n",
      "Epoch: 1800, loss: 0.00054\n",
      "Epoch: 1900, loss: 0.00044\n",
      "Epoch: 2000, loss: 0.00037\n",
      "Epoch: 2100, loss: 0.00032\n",
      "Epoch: 2200, loss: 0.00028\n",
      "Epoch: 2300, loss: 0.00025\n",
      "Epoch: 2400, loss: 0.00023\n",
      "Epoch: 2500, loss: 0.00022\n",
      "Epoch: 2600, loss: 0.00021\n",
      "Epoch: 2700, loss: 0.00020\n",
      "Epoch: 2800, loss: 0.00020\n",
      "Epoch: 2900, loss: 0.00019\n",
      "Epoch: 3000, loss: 0.00019\n",
      "Epoch: 3100, loss: 0.00019\n",
      "Epoch: 3200, loss: 0.00019\n",
      "Epoch: 3300, loss: 0.00018\n",
      "Epoch: 3400, loss: 0.00018\n",
      "Epoch: 3500, loss: 0.00018\n",
      "Epoch: 3600, loss: 0.00018\n",
      "Epoch: 3700, loss: 0.00017\n",
      "Epoch: 3800, loss: 0.00017\n",
      "Epoch: 3900, loss: 0.00017\n",
      "Epoch: 4000, loss: 0.00017\n",
      "Epoch: 4100, loss: 0.00016\n",
      "Epoch: 4200, loss: 0.00016\n",
      "Epoch: 4300, loss: 0.00016\n",
      "Epoch: 4400, loss: 0.00016\n",
      "Epoch: 4500, loss: 0.00016\n",
      "Epoch: 4600, loss: 0.00015\n",
      "Epoch: 4700, loss: 0.00015\n",
      "Epoch: 4800, loss: 0.00015\n",
      "Epoch: 4900, loss: 0.00015\n",
      "Epoch: 5000, loss: 0.00015\n",
      "Epoch: 5100, loss: 0.00014\n",
      "Epoch: 5200, loss: 0.00014\n",
      "Epoch: 5300, loss: 0.00014\n",
      "Epoch: 5400, loss: 0.00014\n",
      "Epoch: 5500, loss: 0.00014\n",
      "Epoch: 5600, loss: 0.00013\n",
      "Epoch: 5700, loss: 0.00013\n",
      "Epoch: 5800, loss: 0.00013\n",
      "Epoch: 5900, loss: 0.00013\n",
      "Epoch: 6000, loss: 0.00013\n",
      "Epoch: 6100, loss: 0.00012\n",
      "Epoch: 6200, loss: 0.00012\n",
      "Epoch: 6300, loss: 0.00012\n",
      "Epoch: 6400, loss: 0.00012\n",
      "Epoch: 6500, loss: 0.00011\n",
      "Epoch: 6600, loss: 0.00011\n",
      "Epoch: 6700, loss: 0.00011\n",
      "Epoch: 6800, loss: 0.00011\n",
      "Epoch: 6900, loss: 0.00011\n",
      "Epoch: 7000, loss: 0.00010\n",
      "Epoch: 7100, loss: 0.00010\n",
      "Epoch: 7200, loss: 0.00010\n",
      "Epoch: 7300, loss: 0.00010\n",
      "Epoch: 7400, loss: 0.00010\n",
      "Epoch: 7500, loss: 0.00009\n",
      "Epoch: 7600, loss: 0.00009\n",
      "Epoch: 7700, loss: 0.00009\n",
      "Epoch: 7800, loss: 0.00009\n",
      "Epoch: 7900, loss: 0.00008\n",
      "Epoch: 8000, loss: 0.00008\n",
      "Epoch: 8100, loss: 0.00008\n",
      "Epoch: 8200, loss: 0.00008\n",
      "Epoch: 8300, loss: 0.00008\n",
      "Epoch: 8400, loss: 0.00007\n",
      "Epoch: 8500, loss: 0.00007\n",
      "Epoch: 8600, loss: 0.00007\n",
      "Epoch: 8700, loss: 0.00007\n",
      "Epoch: 8800, loss: 0.00007\n",
      "Epoch: 8900, loss: 0.00006\n",
      "Epoch: 9000, loss: 0.00006\n",
      "Epoch: 9100, loss: 0.00006\n",
      "Epoch: 9200, loss: 0.00006\n",
      "Epoch: 9300, loss: 0.00005\n",
      "Epoch: 9400, loss: 0.00005\n",
      "Epoch: 9500, loss: 0.00005\n",
      "Epoch: 9600, loss: 0.00005\n",
      "Epoch: 9700, loss: 0.00004\n",
      "Epoch: 9800, loss: 0.00004\n",
      "Epoch: 9900, loss: 0.00004\n",
      "Epoch: 10000, loss: 0.00004\n",
      "Epoch: 10100, loss: 0.00004\n",
      "Epoch: 10200, loss: 0.00004\n",
      "Epoch: 10300, loss: 0.00003\n",
      "Epoch: 10400, loss: 0.00003\n",
      "Epoch: 10500, loss: 0.00003\n",
      "Epoch: 10600, loss: 0.00003\n",
      "Epoch: 10700, loss: 0.00003\n",
      "Epoch: 10800, loss: 0.00003\n",
      "Epoch: 10900, loss: 0.00003\n",
      "Epoch: 11000, loss: 0.00002\n",
      "Epoch: 11100, loss: 0.00002\n",
      "Epoch: 11200, loss: 0.00002\n",
      "Epoch: 11300, loss: 0.00002\n",
      "Epoch: 11400, loss: 0.00002\n",
      "Epoch: 11500, loss: 0.00002\n",
      "Epoch: 11600, loss: 0.00002\n",
      "Epoch: 11700, loss: 0.00002\n",
      "Epoch: 11800, loss: 0.00002\n",
      "Epoch: 11900, loss: 0.00002\n",
      "Epoch: 12000, loss: 0.00002\n",
      "Epoch: 12100, loss: 0.00001\n",
      "Epoch: 12200, loss: 0.00001\n",
      "Epoch: 12300, loss: 0.00001\n",
      "Epoch: 12400, loss: 0.00001\n",
      "Epoch: 12500, loss: 0.00001\n",
      "Epoch: 12600, loss: 0.00001\n",
      "Epoch: 12700, loss: 0.00001\n",
      "Epoch: 12800, loss: 0.00001\n",
      "Epoch: 12900, loss: 0.00001\n",
      "Epoch: 13000, loss: 0.00001\n",
      "Epoch: 13100, loss: 0.00001\n",
      "Epoch: 13200, loss: 0.00001\n",
      "Epoch: 13300, loss: 0.00001\n",
      "Epoch: 13400, loss: 0.00001\n",
      "Epoch: 13500, loss: 0.00001\n",
      "Epoch: 13600, loss: 0.00001\n",
      "Epoch: 13700, loss: 0.00001\n",
      "Epoch: 13800, loss: 0.00001\n",
      "Epoch: 13900, loss: 0.00001\n",
      "Epoch: 14000, loss: 0.00001\n",
      "Epoch: 14100, loss: 0.00001\n",
      "Epoch: 14200, loss: 0.00001\n",
      "Epoch: 14300, loss: 0.00001\n",
      "Epoch: 14400, loss: 0.00001\n",
      "Epoch: 14500, loss: 0.00001\n",
      "Epoch: 14600, loss: 0.00001\n",
      "Epoch: 14700, loss: 0.00001\n",
      "Epoch: 14800, loss: 0.00001\n",
      "Epoch: 14900, loss: 0.00001\n",
      "Epoch: 15000, loss: 0.00001\n",
      "Epoch: 15100, loss: 0.00001\n",
      "Epoch: 15200, loss: 0.00001\n",
      "Epoch: 15300, loss: 0.00001\n",
      "Epoch: 15400, loss: 0.00001\n",
      "Epoch: 15500, loss: 0.00000\n",
      "Epoch: 15600, loss: 0.00000\n",
      "Epoch: 15700, loss: 0.00000\n",
      "Epoch: 15800, loss: 0.00000\n",
      "Epoch: 15900, loss: 0.00000\n",
      "Epoch: 16000, loss: 0.00000\n",
      "Epoch: 16100, loss: 0.00000\n",
      "Epoch: 16200, loss: 0.00000\n",
      "Epoch: 16300, loss: 0.00000\n",
      "Epoch: 16400, loss: 0.00000\n",
      "Epoch: 16500, loss: 0.00000\n",
      "Epoch: 16600, loss: 0.00000\n",
      "Epoch: 16700, loss: 0.00000\n",
      "Epoch: 16800, loss: 0.00000\n",
      "Epoch: 16900, loss: 0.00000\n",
      "Epoch: 17000, loss: 0.00000\n",
      "Epoch: 17100, loss: 0.00000\n",
      "Epoch: 17200, loss: 0.00000\n",
      "Epoch: 17300, loss: 0.00000\n",
      "Epoch: 17400, loss: 0.00000\n",
      "Epoch: 17500, loss: 0.00000\n",
      "Epoch: 17600, loss: 0.00000\n",
      "Epoch: 17700, loss: 0.00000\n",
      "Epoch: 17800, loss: 0.00000\n",
      "Epoch: 17900, loss: 0.00000\n",
      "Epoch: 18000, loss: 0.00000\n",
      "Epoch: 18100, loss: 0.00000\n",
      "Epoch: 18200, loss: 0.00000\n",
      "Epoch: 18300, loss: 0.00000\n",
      "Epoch: 18400, loss: 0.00000\n",
      "Epoch: 18500, loss: 0.00000\n",
      "Epoch: 18600, loss: 0.00000\n",
      "Epoch: 18700, loss: 0.00000\n",
      "Epoch: 18800, loss: 0.00000\n",
      "Epoch: 18900, loss: 0.00000\n",
      "Epoch: 19000, loss: 0.00000\n",
      "Epoch: 19100, loss: 0.00000\n",
      "Epoch: 19200, loss: 0.00000\n",
      "Epoch: 19300, loss: 0.00000\n",
      "Epoch: 19400, loss: 0.00000\n",
      "Epoch: 19500, loss: 0.00000\n",
      "Epoch: 19600, loss: 0.00000\n",
      "Epoch: 19700, loss: 0.00000\n",
      "Epoch: 19800, loss: 0.00000\n",
      "Epoch: 19900, loss: 0.00000\n",
      "Epoch: 20000, loss: 0.00000\n",
      "Epoch: 20100, loss: 0.00000\n",
      "Epoch: 20200, loss: 0.00000\n",
      "Epoch: 20300, loss: 0.00000\n",
      "Epoch: 20400, loss: 0.00000\n",
      "Epoch: 20500, loss: 0.00000\n",
      "Epoch: 20600, loss: 0.00000\n",
      "Epoch: 20700, loss: 0.00000\n",
      "Epoch: 20800, loss: 0.00000\n",
      "Epoch: 20900, loss: 0.00000\n",
      "Epoch: 21000, loss: 0.00000\n",
      "Epoch: 21100, loss: 0.00000\n",
      "Epoch: 21200, loss: 0.00000\n",
      "Epoch: 21300, loss: 0.00000\n",
      "Epoch: 21400, loss: 0.00000\n",
      "Epoch: 21500, loss: 0.00000\n",
      "Epoch: 21600, loss: 0.00000\n",
      "Epoch: 21700, loss: 0.00000\n",
      "Epoch: 21800, loss: 0.00000\n",
      "Epoch: 21900, loss: 0.00000\n",
      "Epoch: 22000, loss: 0.00000\n",
      "Epoch: 22100, loss: 0.00000\n",
      "Epoch: 22200, loss: 0.00000\n",
      "Epoch: 22300, loss: 0.00000\n",
      "Epoch: 22400, loss: 0.00000\n",
      "Epoch: 22500, loss: 0.00000\n",
      "Epoch: 22600, loss: 0.00000\n",
      "Epoch: 22700, loss: 0.00000\n",
      "Epoch: 22800, loss: 0.00000\n",
      "Epoch: 22900, loss: 0.00000\n",
      "Epoch: 23000, loss: 0.00000\n",
      "Epoch: 23100, loss: 0.00000\n",
      "Epoch: 23200, loss: 0.00000\n",
      "Epoch: 23300, loss: 0.00000\n",
      "Epoch: 23400, loss: 0.00000\n",
      "Epoch: 23500, loss: 0.00000\n",
      "Epoch: 23600, loss: 0.00000\n",
      "Epoch: 23700, loss: 0.00000\n",
      "Epoch: 23800, loss: 0.00000\n",
      "Epoch: 23900, loss: 0.00000\n",
      "Epoch: 24000, loss: 0.00000\n",
      "Epoch: 24100, loss: 0.00000\n",
      "Epoch: 24200, loss: 0.00000\n",
      "Epoch: 24300, loss: 0.00000\n",
      "Epoch: 24400, loss: 0.00000\n",
      "Epoch: 24500, loss: 0.00000\n",
      "Epoch: 24600, loss: 0.00000\n",
      "Epoch: 24700, loss: 0.00000\n",
      "Epoch: 24800, loss: 0.00000\n",
      "Epoch: 24900, loss: 0.00000\n",
      "Epoch: 25000, loss: 0.00000\n",
      "Epoch: 25100, loss: 0.00000\n",
      "Epoch: 25200, loss: 0.00000\n",
      "Epoch: 25300, loss: 0.00000\n",
      "Epoch: 25400, loss: 0.00000\n",
      "Epoch: 25500, loss: 0.00000\n",
      "Epoch: 25600, loss: 0.00000\n",
      "Epoch: 25700, loss: 0.00000\n",
      "Epoch: 25800, loss: 0.00000\n",
      "Epoch: 25900, loss: 0.00000\n",
      "Epoch: 26000, loss: 0.00000\n",
      "Epoch: 26100, loss: 0.00000\n",
      "Epoch: 26200, loss: 0.00000\n",
      "Epoch: 26300, loss: 0.00000\n",
      "Epoch: 26400, loss: 0.00000\n",
      "Epoch: 26500, loss: 0.00000\n",
      "Epoch: 26600, loss: 0.00000\n",
      "Epoch: 26700, loss: 0.00000\n",
      "Epoch: 26800, loss: 0.00000\n",
      "Epoch: 26900, loss: 0.00000\n",
      "Epoch: 27000, loss: 0.00000\n",
      "Epoch: 27100, loss: 0.00000\n",
      "Epoch: 27200, loss: 0.00000\n",
      "Epoch: 27300, loss: 0.00000\n",
      "Epoch: 27400, loss: 0.00000\n",
      "Epoch: 27500, loss: 0.00000\n",
      "Epoch: 27600, loss: 0.00000\n",
      "Epoch: 27700, loss: 0.00000\n",
      "Epoch: 27800, loss: 0.00000\n",
      "Epoch: 27900, loss: 0.00000\n",
      "Epoch: 28000, loss: 0.00000\n",
      "Epoch: 28100, loss: 0.00000\n",
      "Epoch: 28200, loss: 0.00000\n",
      "Epoch: 28300, loss: 0.00000\n",
      "Epoch: 28400, loss: 0.00000\n",
      "Epoch: 28500, loss: 0.00000\n",
      "Epoch: 28600, loss: 0.00000\n",
      "Epoch: 28700, loss: 0.00000\n",
      "Epoch: 28800, loss: 0.00000\n",
      "Epoch: 28900, loss: 0.00000\n",
      "Epoch: 29000, loss: 0.00000\n",
      "Epoch: 29100, loss: 0.00000\n",
      "Epoch: 29200, loss: 0.00000\n",
      "Epoch: 29300, loss: 0.00000\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    outputs = lstm1.forward(X_train_tensors_final.to(device)) #forward pass \n",
    "    optimizer.zero_grad() #caluclate the gradient, manually setting to 0 \n",
    "    # obtain the loss function \n",
    "    loss = loss_function(outputs, y_train_tensors.to(device)) \n",
    "    loss.backward() #calculates the loss of the loss function \n",
    "    optimizer.step() #improve from loss, i.e backprop \n",
    "    if epoch % 100 == 0: \n",
    "        print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_ss = ss.transform(df.drop(columns='Volume'))\n",
    "df_y_mm = mm.transform(df.iloc[:, 4:5])\n",
    "\n",
    "df_X_ss = Variable(torch.Tensor(df_X_ss)) #converting to Tensors\n",
    "df_y_mm = Variable(torch.Tensor(df_y_mm))\n",
    "\n",
    "#reshaping the dataset\n",
    "df_X_ss = torch.reshape(df_X_ss, (df_X_ss.shape[0], 1, df_X_ss.shape[1]))\n",
    "train_predict = lstm1(df_X_ss.to(device))#forward pass\n",
    "data_predict = train_predict.data.detach().cpu().numpy() #numpy conversion\n",
    "dataY_plot = df_y_mm.data.numpy()\n",
    "\n",
    "data_predict = mm.inverse_transform(data_predict) #reverse transformation\n",
    "dataY_plot = mm.inverse_transform(dataY_plot)\n",
    "plt.figure(figsize=(10,6)) #plotting\n",
    "plt.axvline(x=cut_line, c='r', linestyle='--') #size of the training set\n",
    "\n",
    "plt.plot(dataY_plot, label='actual price') #actual plot\n",
    "plt.plot(data_predict, label='predicted price') #predicted plot\n",
    "plt.title(fn[num].split(\".\")[0])\n",
    "plt.legend()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
